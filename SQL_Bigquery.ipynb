{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SQL Bigquery.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "To use BigQuery, we'll import the Python package below:"
      ],
      "metadata": {
        "id": "--yb55aPdQ3i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szQWF7Y_Ulp4"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step in the workflow is to create a Client object. As you'll soon see, this Client object will play a central role in retrieving information from BigQuery datasets."
      ],
      "metadata": {
        "id": "QS8ORUz6E2lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a \"Client\" object\n",
        "client = bigquery.Client()"
      ],
      "metadata": {
        "id": "7YqjSHbbU1LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll work with a dataset of posts on Hacker News, a website focusing on computer science and cybersecurity news.\n",
        "\n",
        "In BigQuery, each dataset is contained in a corresponding project. In this case, our hacker_news dataset is contained in the bigquery-public-data project. To access the dataset,\n",
        "\n",
        "We begin by constructing a reference to the dataset with the dataset() method.\n",
        "Next, we use the get_dataset() method, along with the reference we just constructed, to fetch the dataset."
      ],
      "metadata": {
        "id": "Tkd1gLG5FFw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a reference to the \"hacker_news\" dataset\n",
        "dataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n",
        "\n",
        "# API request - fetch the dataset\n",
        "dataset = client.get_dataset(dataset_ref)"
      ],
      "metadata": {
        "id": "-hOny92kU1OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every dataset is just a collection of tables. You can think of a dataset as a spreadsheet file containing multiple tables, all composed of rows and columns.\n",
        "\n",
        "We use the list_tables() method to list the tables in the dataset."
      ],
      "metadata": {
        "id": "zeQrJgnlFqL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List all the tables in the \"hacker_news\" dataset\n",
        "tables = list(client.list_tables(dataset))"
      ],
      "metadata": {
        "id": "PSf2Yke7U1RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print names of all tables in the dataset (there are four!)\n",
        "for table in tables:  \n",
        "    print(table.table_id)"
      ],
      "metadata": {
        "id": "dGsXyllBU1SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to how we fetched a dataset, we can fetch a table. In the code cell below, we fetch the full table in the hacker_news dataset."
      ],
      "metadata": {
        "id": "z-nu_rbwFrM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct a reference to the \"full\" table\n",
        "table_ref = dataset_ref.table(\"full\")"
      ],
      "metadata": {
        "id": "2jQkni7WU1YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API request - fetch the table\n",
        "table = client.get_table(table_ref)"
      ],
      "metadata": {
        "id": "_GEU7rxaVt8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The structure of a table is called its schema. We need to understand a table's schema to effectively pull out the data we want.\n",
        "\n",
        "In this example, we'll investigate the full table that we fetched above."
      ],
      "metadata": {
        "id": "mGbyUetLGA4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print information on all the columns in the \"full\" table in the \"hacker_news\" dataset\n",
        "table.schema"
      ],
      "metadata": {
        "id": "yZ7aGeabVt_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the list_rows() method to check just the first five lines of of the full table to make sure this is right. (Sometimes databases have outdated descriptions, so it's good to check.) This returns a BigQuery RowIterator object that can quickly be converted to a pandas DataFrame with the to_dataframe() method."
      ],
      "metadata": {
        "id": "L6Gqb5EVdqf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview the first five lines of the \"full\" table\n",
        "client.list_rows(table, max_results=5).to_dataframe()"
      ],
      "metadata": {
        "id": "n8PZ0ujKVuCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The list_rows() method will also let us look at just the information in a specific column. If we want to see the first five entries in the titel column, for example, we can do that!"
      ],
      "metadata": {
        "id": "CKaIICivdvdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview the first five entries in the \"title\" column of the \"full\" table\n",
        "client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()"
      ],
      "metadata": {
        "id": "SmwevQsyVuFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query to select all the items from the \"city\" column where the \"country\" column is 'US'\n",
        "query = \"\"\"\n",
        "        SELECT city\n",
        "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
        "        WHERE country = 'US'\n",
        "        \"\"\""
      ],
      "metadata": {
        "id": "qFtZ-R7jZfQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Submitting the query to the dataset**\n",
        "We begin by setting up the query with the query() method. "
      ],
      "metadata": {
        "id": "2bdR_kSmZbf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the query\n",
        "query_job = client.query(query)"
      ],
      "metadata": {
        "id": "kk2TfC7-Zm9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we run the query and convert the results to a pandas DataFrame."
      ],
      "metadata": {
        "id": "hji2k19lZq5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# API request - run the query, and return a pandas DataFrame\n",
        "us_cities = query_job.to_dataframe()"
      ],
      "metadata": {
        "id": "t6MvBRq_Z3n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've got a pandas DataFrame called us_cities, which we can use like any other DataFrame."
      ],
      "metadata": {
        "id": "EVJN8S3zZmn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What five cities have the most measurements?\n",
        "us_cities.city.value_counts().head()"
      ],
      "metadata": {
        "id": "QHtKAQpVZ6nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Working with big datasets**\n",
        "To begin,you can estimate the size of any query before running it. Here is an example using the (very large!) Hacker News dataset. To see how much data a query will scan, we create a QueryJobConfig object and set the dry_run parameter to True.\n",
        "\n"
      ],
      "metadata": {
        "id": "wFkL8nJVaD47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query to get the score column from every row where the type column has value \"job\"\n",
        "query = \"\"\"\n",
        "        SELECT score, title\n",
        "        FROM `bigquery-public-data.hacker_news.full`\n",
        "        WHERE type = \"job\" \n",
        "        \"\"\"\n",
        "\n",
        "# Create a QueryJobConfig object to estimate size of query without running it\n",
        "dry_run_config = bigquery.QueryJobConfig(dry_run=True)\n",
        "\n",
        "# API request - dry run query to estimate costs\n",
        "dry_run_query_job = client.query(query, job_config=dry_run_config)\n",
        "\n",
        "print(\"This query will process {} bytes.\".format(dry_run_query_job.total_bytes_processed))"
      ],
      "metadata": {
        "id": "2N9dvSIgaI51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also specify a parameter when running the query to limit how much data you are willing to scan. Here's an example with a low limit."
      ],
      "metadata": {
        "id": "osU7Irm8aRwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only run the query if it's less than 1 MB\n",
        "ONE_MB = 1000*1000\n",
        "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_MB)\n",
        "\n",
        "# Set up the query (will only run if it's less than 1 MB)\n",
        "safe_query_job = client.query(query, job_config=safe_config)\n",
        "\n",
        "# API request - try to run the query, and return a pandas DataFrame\n",
        "safe_query_job.to_dataframe()"
      ],
      "metadata": {
        "id": "7-gtUoXmaStg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the query was cancelled, because the limit of 1 MB was exceeded. However, we can increase the limit to run the query successfully!"
      ],
      "metadata": {
        "id": "81Txd02taLpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only run the query if it's less than 1 GB\n",
        "ONE_GB = 1000*1000*1000\n",
        "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_GB)\n",
        "\n",
        "# Set up the query (will only run if it's less than 1 GB)\n",
        "safe_query_job = client.query(query, job_config=safe_config)\n",
        "\n",
        "# API request - try to run the query, and return a pandas DataFrame\n",
        "job_post_scores = safe_query_job.to_dataframe()\n",
        "\n",
        "# Print average score for job posts\n",
        "job_post_scores.score.mean()"
      ],
      "metadata": {
        "id": "tPiipj38aXki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are ever unsure what to put inside the COUNT() function, you can do COUNT(1) to count the rows in each group. Most people find it especially readable, because we know it's not focusing on other columns. It also scans less data than if supplied column names (making it faster and using less of your data access quota)."
      ],
      "metadata": {
        "id": "6betuZIRfB4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that because it tells SQL how to apply aggregate functions (like COUNT()), it doesn't make sense to use GROUP BY without an aggregate function. Similarly, if you have any GROUP BY clause, then all variables must be passed to either a\n",
        "\n",
        "GROUP BY command, or\n",
        "an aggregation function."
      ],
      "metadata": {
        "id": "dfXaOjyWfCno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "H_z1xY_tfFKK"
      }
    }
  ]
}